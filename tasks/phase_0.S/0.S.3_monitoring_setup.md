# Task 0.S.3: Monitoring & Observability Setup

**Phase:** 0.S (Shared Infrastructure)  
**Owner:** workflow-engine team, PlayRAGNA team (dashboard design)  
**Effort:** 2 hours  
**Priority:** 🟠 HIGH (Operations critical)  
**Status:** ⏳ PENDING

---

## 🎯 OBJECTIVE

Configure Prometheus + Grafana monitoring stack for FlowForge shared infrastructure observability.

---

## ✅ SUB-TASKS

### Prometheus Configuration
- [ ] Create `prometheus/prometheus.yml` scrape configs (30 min)
- [ ] Configure metrics endpoints (Ollama, RAGNA) (15 min)
- [ ] Setup alert rules (service down, high latency) (30 min)

### Grafana Configuration
- [ ] Create unified dashboard JSON (Ollama + RAGNA + System) (1h)
- [ ] Configure data source (Prometheus) (5 min)
- [ ] Setup email/Slack alerting (optional) (10 min)

---

## 📋 DELIVERABLES

```yaml
shared-infrastructure/
├── prometheus/
│   ├── prometheus.yml        # Scrape configs
│   └── alerts.yml            # Alert rules
├── grafana/
│   ├── dashboards/
│   │   └── flowforge-unified.json
│   └── datasources/
│       └── prometheus.yml
└── README.md                  # Monitoring guide + troubleshooting
```

---

## 📊 PROMETHEUS CONFIGURATION

**Scrape Configuration:**

```yaml
# prometheus/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'ollama'
    static_configs:
      - targets: ['ollama:11434']
    metrics_path: '/metrics'
    
  - job_name: 'ragna'
    static_configs:
      - targets: ['ragna:31476']
    metrics_path: '/metrics'
    
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

rule_files:
  - 'alerts.yml'

alerting:
  alertmanagers:
    - static_configs:
        - targets: ['alertmanager:9093']  # Optional
```

**Alert Rules:**

```yaml
# prometheus/alerts.yml
groups:
  - name: flowforge_alerts
    interval: 30s
    rules:
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "{{ $labels.job }} has been down for 1 minute"
      
      - alert: HighLatency
        expr: http_request_duration_seconds{quantile="0.95"} > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency on {{ $labels.job }}"
          description: "P95 latency > 5s for 5 minutes"
      
      - alert: HighMemoryUsage
        expr: container_memory_usage_bytes / container_spec_memory_limit_bytes > 0.9
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ $labels.container_name }}"
          description: "Memory usage > 90% for 2 minutes"
```

---

## 📊 GRAFANA DASHBOARD SPECIFICATION

**Unified Dashboard Panels:**

### Row 1: Ollama Service
- **Panel 1.1:** Requests per Second (rate[5m])
- **Panel 1.2:** P95 Latency (histogram_quantile)
- **Panel 1.3:** Active Connections (gauge)
- **Panel 1.4:** Model Loaded Status (boolean)

### Row 2: RAGNA Service
- **Panel 2.1:** Queries per Second
- **Panel 2.2:** RAG Pipeline Duration (P50/P95/P99)
- **Panel 2.3:** Document Index Size (gauge)
- **Panel 2.4:** Cache Hit Rate (percentage)

### Row 3: System Resources
- **Panel 3.1:** CPU Usage (%) - per service
- **Panel 3.2:** Memory Usage (GB) - per service
- **Panel 3.3:** Disk I/O (MB/s)
- **Panel 3.4:** Network Traffic (Mbps)

### Row 4: GPU Metrics (conditional)
- **Panel 4.1:** GPU Utilization (%)
- **Panel 4.2:** GPU Memory Usage (GB)
- **Panel 4.3:** GPU Temperature (°C)
- **Panel 4.4:** GPU Power Draw (W)

**Dashboard JSON Structure:**

```json
{
  "dashboard": {
    "title": "FlowForge Unified Monitoring",
    "tags": ["flowforge", "shared-infrastructure"],
    "timezone": "browser",
    "panels": [
      {
        "title": "Ollama Requests/sec",
        "targets": [
          {
            "expr": "rate(http_requests_total{job='ollama'}[5m])"
          }
        ]
      }
      // ... additional panels
    ]
  }
}
```

---

## 🎯 ACCEPTANCE CRITERIA

### Functional Requirements
- [ ] Prometheus scrapes metrics from Ollama and RAGNA every 15s
- [ ] Grafana displays live data from Prometheus
- [ ] Alert rules fire correctly (test with service shutdown)
- [ ] Dashboard accessible at `http://localhost:3000`
- [ ] Data retention: 15 days (default)

### Dashboard Requirements
- [ ] All 4 rows visible without scrolling (1080p resolution)
- [ ] Auto-refresh enabled (30s interval)
- [ ] Time range selector functional (last 1h default)
- [ ] Panels show "No Data" gracefully when services offline
- [ ] GPU panels conditionally hidden on CPU-only systems

### Alert Requirements
- [ ] ServiceDown fires within 1 minute of service crash
- [ ] HighLatency detects 5s+ latency for 5 minutes
- [ ] HighMemoryUsage detects 90%+ usage for 2 minutes
- [ ] Alerts visible in Grafana Alerting tab
- [ ] Email/Slack integration (optional, documented)

---

## 🔗 DEPENDENCIES

**Blocks:**
- None (monitoring is non-blocking)

**Blocked By:**
- [0.S.1: Docker Compose Setup](0.S.1_docker_compose_setup.md) - Prometheus/Grafana must be running

**Related Tasks:**
- [0.S.2: GPU Configuration](0.S.2_gpu_configuration.md) - GPU metrics panels

---

## 📊 METRICS

**Monitoring Targets:**

| Service | Metric | Target | Alert Threshold |
|---------|--------|--------|-----------------|
| Ollama | Latency P95 | <1s (GPU) | >5s for 5min |
| Ollama | Throughput | 60+ req/min | N/A |
| RAGNA | Query Time | <2s | >10s for 5min |
| RAGNA | Uptime | 99.5%+ | Down for 1min |
| System | Memory | <90% | >90% for 2min |
| GPU | Utilization | 60-80% | >95% for 5min |

**Observability Goals:**
- Incident detection: <1 minute
- Root cause identification: <5 minutes
- Historical analysis: 15 days retention
- Zero blind spots: All services monitored

---

## 🔍 TESTING PLAN

### Smoke Tests
```bash
# Test 1: Prometheus scraping
curl http://localhost:9090/api/v1/targets
# Expected: All targets "up" state

# Test 2: Metrics available
curl http://localhost:9090/api/v1/query?query=up
# Expected: {"status":"success",...}

# Test 3: Grafana data source
curl -u admin:admin http://localhost:3000/api/datasources
# Expected: Prometheus datasource configured
```

### Alert Tests
```bash
# Test 1: ServiceDown alert
docker-compose stop ollama
# Wait 1 minute
# Expected: Alert fires in Grafana

# Test 2: HighLatency alert
# Trigger slow query to Ollama
# Expected: Alert fires after 5 minutes

# Test 3: Recovery
docker-compose start ollama
# Expected: Alert resolves automatically
```

### Dashboard Tests
- All panels load data successfully
- Time range selector changes data correctly
- Auto-refresh updates panels every 30s
- GPU panels appear only on GPU-enabled systems
- Export dashboard JSON works

---

## 📝 IMPLEMENTATION NOTES

**Metrics Collection Strategy:**

```yaml
Ollama Metrics:
  Endpoint: /metrics (Prometheus format)
  Instrumentation: Built-in (Ollama native)
  Custom Metrics: None required
  
RAGNA Metrics:
  Endpoint: /metrics (Prometheus format)
  Instrumentation: May need custom (check RAGNA capabilities)
  Fallback: Health check endpoint polling
  
System Metrics:
  Source: cAdvisor (Docker container metrics)
  Integration: Prometheus scrapes Docker API
  Panels: CPU, Memory, Network, Disk
```

**Dashboard Design Principles:**
- One screen overview (no scrolling on 1080p)
- Traffic light colors (green/yellow/red)
- Clear units (req/s, ms, GB, %)
- Contextual tooltips on panels
- Responsive layout (works on tablet/phone)

**Alert Tuning:**
- Avoid alert fatigue (only actionable alerts)
- Appropriate thresholds (based on SLOs)
- Clear runbook links in annotations
- Escalation tiers (warning → critical)

---

## 🔄 UPDATE LOG

| Date | Status | Notes |
|------|--------|-------|
| 2025-01-16 | Created | Task extracted from MASTER-ROADMAP_PHASE_0.S_SHARED_INFRA.md |

---

**Parent Document:** [MASTER-ROADMAP.md](../../MASTER-ROADMAP.md#week-1)  
**Phase Documentation:** [MASTER-ROADMAP_PHASE_0.S_SHARED_INFRA.md](../../workflow-engine/.windsurf/MASTER-ROADMAP_PHASE_0.S_SHARED_INFRA.md)  
**Previous Task:** [0.S.2: GPU Configuration](0.S.2_gpu_configuration.md)  
**Completes:** Phase 0.S (Shared Infrastructure Setup)
