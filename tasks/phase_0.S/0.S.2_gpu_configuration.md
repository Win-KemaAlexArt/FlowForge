# Task 0.S.2: GPU Acceleration Configuration

**Phase:** 0.S (Shared Infrastructure)  
**Owner:** workflow-engine team  
**Effort:** 1 hour  
**Priority:** üî¥ CRITICAL (10x performance improvement)  
**Status:** ‚è≥ PENDING

---

## üéØ OBJECTIVE

Add GPU acceleration support to Ollama service for 10x performance improvement (5-10s ‚Üí 0.5-1s inference latency).

---

## ‚úÖ SUB-TASKS

### GPU Detection & Configuration
- [x] Create `scripts/detect-gpu.sh` script (20 min)
- [x] Add GPU configuration to `docker-compose.yml` (10 min)
- [x] Update `install.sh` to conditionally enable GPU (15 min)
- [x] Test GPU acceleration on NVIDIA system (10 min)
- [x] Document fallback behavior for non-GPU systems (5 min)

---

## üìã DELIVERABLES

```yaml
shared-infrastructure/
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ detect-gpu.sh        # Auto-detect NVIDIA GPU availability
‚îú‚îÄ‚îÄ docker-compose.yml        # Updated with GPU config (conditional)
‚îî‚îÄ‚îÄ README.md                 # GPU setup instructions + troubleshooting
```

---

## üê≥ DOCKER COMPOSE GPU CONFIGURATION

**GPU-Enabled Ollama Service:**

```yaml
services:
  ollama:
    image: ollama/ollama:latest
    ports: ["11434:11434"]
    volumes: ["ollama-data:/root/.ollama"]
    # GPU Configuration (conditional)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck: 
      test: "curl -f http://localhost:11434/api/tags"
```

**GPU Detection Script:**

```bash
#!/bin/bash
# scripts/detect-gpu.sh

if command -v nvidia-smi &> /dev/null; then
    echo "‚úÖ NVIDIA GPU detected"
    nvidia-smi --query-gpu=name --format=csv,noheader
    
    # Enable GPU in docker-compose
    sed -i 's/# GPU_ENABLED=0/GPU_ENABLED=1/' .env
    exit 0
else
    echo "‚ö†Ô∏è  No NVIDIA GPU found - falling back to CPU"
    echo "   Performance: 5-10s inference (vs 0.5-1s GPU)"
    exit 1
fi
```

---

## üéØ ACCEPTANCE CRITERIA

### Functional Requirements
- [ ] GPU auto-detected on Windows/Linux NVIDIA systems
- [ ] Ollama inference <1s with GPU enabled
- [ ] `nvidia-smi` visible inside Ollama container (GPU systems)
- [ ] Graceful fallback to CPU on macOS/AMD systems
- [ ] Zero manual configuration required from user

### Performance Requirements
- [ ] **GPU Systems:** Inference 0.5-1s (10x improvement)
- [ ] **CPU Systems:** Inference 5-10s (baseline)
- [ ] Throughput: 60-120 req/min (GPU) vs 6-12 req/min (CPU)
- [ ] Memory: 4GB allocated (GPU) vs 2GB (CPU)

### Platform Support
- [ ] **Windows NVIDIA:** GPU enabled ‚úÖ
- [ ] **Linux NVIDIA:** GPU enabled ‚úÖ
- [ ] **macOS Apple Silicon:** CPU fallback (Docker limitation)
- [ ] **AMD GPU:** CPU fallback (Ollama uses CUDA only)

---

## üîó DEPENDENCIES

**Blocks:**
- [PlayRAGNA Phase 3.2](../playragna/phase_3.2_ragna_integration.md) - Benefits from 10x faster Ollama
- [workflow-engine Phase 0.3](../workflow-engine/phase_0.3_cli_core.md) - Benefits from 10x faster Ollama

**Blocked By:**
- [0.S.1: Docker Compose Setup](0.S.1_docker_compose_setup.md) - Must exist first

**Related Tasks:**
- [0.S.1: Docker Compose Setup](0.S.1_docker_compose_setup.md) - Base infrastructure
- [0.S.3: Monitoring Setup](0.S.3_monitoring_setup.md) - GPU metrics monitoring

---

## üìä METRICS

**Performance Impact:**

| Metric | CPU (Before) | GPU (After) | Improvement |
|--------|--------------|-------------|-------------|
| Inference Latency | 5-10s | 0.5-1s | **10x faster** |
| Throughput | 6-12 req/min | 60-120 req/min | **10x capacity** |
| Concurrent Users | 1-2 | 10-20 | **10x scale** |
| Model Load Time | 30-60s | 5-10s | **6x faster** |

**Resource Usage:**

| Resource | CPU Mode | GPU Mode |
|----------|----------|----------|
| RAM | 2GB | 4GB |
| VRAM | 0GB | 4-8GB |
| CPU Usage | 80-100% | 10-20% |
| GPU Usage | 0% | 60-80% |

---

## üîç TESTING PLAN

### GPU Detection Tests
```bash
# Test 1: Detect GPU on NVIDIA system
./scripts/detect-gpu.sh
# Expected: "‚úÖ NVIDIA GPU detected" + GPU name

# Test 2: Verify GPU in container
docker exec flowforge-ollama-1 nvidia-smi
# Expected: GPU device list + memory info

# Test 3: Fallback behavior (non-GPU system)
# Expected: "‚ö†Ô∏è No NVIDIA GPU found - falling back to CPU"
```

### Performance Benchmarks
```bash
# Benchmark: Inference speed (GPU)
time curl -X POST http://localhost:11434/api/generate \
  -d '{"model":"mistral:7b","prompt":"Hello world"}'
# Target: <1s total time

# Benchmark: Throughput test
for i in {1..60}; do
  curl -X POST http://localhost:11434/api/generate \
    -d '{"model":"mistral:7b","prompt":"Test '$i'"}' &
done
wait
# Target: All 60 requests complete in <60s (1 req/sec)
```

### Integration Tests
- PlayRAGNA chat response time <2s end-to-end
- workflow-engine workflow generation <5s
- No GPU memory overflow with concurrent requests
- CPU fallback works without errors

---

## üìù IMPLEMENTATION NOTES

**Research Foundation:**
- GPU optimization identified in [Priority Matrix](../../workflow-engine/docs/Priority_Matrix_Docker_MCP.md) (Recommendation #2, 79/100 score)
- Performance benchmarks from [Docker MCP Research Part1](../../workflow-engine/docs/Docker_Desktop_MCP_Research_Part1.md)

**Platform Limitations:**
```yaml
macOS Apple Silicon:
  Issue: Docker Desktop doesn't support GPU passthrough
  Workaround: CPU fallback (acceptable UX for macOS users)
  Impact: 5-10s inference (vs 0.5-1s GPU)
  
AMD GPUs:
  Issue: Ollama uses CUDA (NVIDIA only)
  Workaround: CPU fallback
  Future: ROCm support (Ollama roadmap)
```

**Configuration Strategy:**
- Automatic detection (no user config required)
- Graceful degradation (CPU fallback works)
- Clear messaging (users know their performance tier)
- Future-proof (easy to add ROCm when available)

---

## üîÑ UPDATE LOG

| Date | Status | Notes |
|------|--------|-------|
| 2025-01-16 | Created | Task extracted from MASTER-ROADMAP_PHASE_0.S_SHARED_INFRA.md |

---

**Parent Document:** [MASTER-ROADMAP.md](../../MASTER-ROADMAP.md#week-1)  
**Phase Documentation:** [MASTER-ROADMAP_PHASE_0.S_SHARED_INFRA.md](../../workflow-engine/.windsurf/MASTER-ROADMAP_PHASE_0.S_SHARED_INFRA.md)  
**Previous Task:** [0.S.1: Docker Compose Setup](0.S.1_docker_compose_setup.md)  
**Next Task:** [0.S.3: Monitoring Setup](0.S.3_monitoring_setup.md)
